# æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶æ·±åº¦åˆ†æä¸ä¼˜åŒ–æŠ¥å‘Š

## ğŸ“‹ æŠ¥å‘Šæ¦‚è¿°

**åˆ†æå¯¹è±¡**: exp_forecasting_ml_optim.py (2761è¡Œ)  
**åˆ†ææ—¶é—´**: 2026-02-11  
**æ¡†æ¶ç±»å‹**: åŸºäºæœºå™¨å­¦ä¹ çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶  
**æ”¯æŒæ–¹æ³•**: 7ç§é¢„æµ‹ç­–ç•¥ï¼ˆUSMDO, USMD, USMR, USMDR, MSMD, MSMR, MSMDRï¼‰

---

## 1. å½“å‰æ¡†æ¶ä¼˜åŠ¿åˆ†æ âœ…

### 1.1 æ¶æ„è®¾è®¡ä¼˜åŠ¿

âœ… **æ¨¡å—åŒ–è®¾è®¡æ¸…æ™°**
- ModelConfigç±»åˆ†ç¦»é…ç½®
- FeaturePreprocessorç‹¬ç«‹å¤„ç†ç‰¹å¾
- Modelç±»å°è£…æ ¸å¿ƒé€»è¾‘

âœ… **æ”¯æŒå¤šç§é¢„æµ‹ç­–ç•¥**
- è¦†ç›–å•å˜é‡/å¤šå˜é‡
- è¦†ç›–ç›´æ¥/é€’å½’/æ··åˆç­–ç•¥
- æä¾›çµæ´»çš„æ–¹æ³•é€‰æ‹©

âœ… **å®Œæ•´çš„åŠŸèƒ½é—­ç¯**
- æ•°æ®åŠ è½½ â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ¨¡å‹è®­ç»ƒ â†’ æµ‹è¯•éªŒè¯ â†’ é¢„æµ‹è¾“å‡º
- æ¯ä¸ªç¯èŠ‚éƒ½æœ‰è¯¦ç»†å®ç°

### 1.2 ç‰¹å¾å·¥ç¨‹ä¼˜åŠ¿

âœ… **æ—¶é—´ç‰¹å¾ä¸°å¯Œ**
- å°æ—¶ã€æ˜ŸæœŸã€æœˆä»½ã€å­£åº¦ç­‰
- å‘¨æœŸæ€§ç¼–ç ï¼ˆsin/cosï¼‰
- èŠ‚å‡æ—¥æ ‡è®°

âœ… **æ»åç‰¹å¾**
- æ”¯æŒè‡ªå®šä¹‰æ»åæœŸ
- å•å˜é‡å’Œå¤šå˜é‡æ»å

âœ… **å¤–ç”Ÿå˜é‡æ”¯æŒ**
- å¤©æ°”æ•°æ®é›†æˆ
- æ—¥æœŸç±»å‹æ•°æ®é›†æˆ
- çµæ´»çš„å¤–ç”Ÿå˜é‡æ¡†æ¶

---

## 2. å­˜åœ¨çš„é—®é¢˜å’Œä¸è¶³ âš ï¸

### 2.1 ä»£ç ç»“æ„é—®é¢˜

#### é—®é¢˜1: æ¨¡å‹ç¡¬ç¼–ç ï¼ˆä¸¥é‡ï¼‰âš ï¸âš ï¸âš ï¸

**é—®é¢˜æè¿°:**
```python
# è¡Œ661-677: æ¨¡å‹å‚æ•°ç¡¬ç¼–ç ä¸ºLightGBM
self.model_params = {
    "boosting_type": "gbdt",
    "objective": self.args.objective,
    # ... LightGBMç‰¹æœ‰å‚æ•°
}

# è¡Œ1586: æ¨¡å‹åˆ›å»ºç¡¬ç¼–ç 
lgbm_estimator = lgb.LGBMRegressor(**self.model_params)
```

**å½±å“:**
- æ— æ³•è½»æ¾åˆ‡æ¢åˆ°XGBoostã€CatBoostæˆ–å…¶ä»–æ¨¡å‹
- è¿åå¼€é—­åŸåˆ™
- æ‰©å±•æ€§å·®

**ä¼˜å…ˆçº§:** P0ï¼ˆå¿…é¡»ä¿®å¤ï¼‰

---

#### é—®é¢˜2: ä»£ç é‡å¤ä¸¥é‡ ğŸ”„

**é‡å¤ä»£ç ç»Ÿè®¡:**

1. **ç‰¹å¾ç¼©æ”¾ä»£ç é‡å¤7+æ¬¡**
```python
# åœ¨æ¯ä¸ªé¢„æµ‹æ–¹æ³•ä¸­éƒ½é‡å¤
if scaler_features is not None:
    if self.args.encode_categorical_features:
        categorical_features = [...]
        numeric_features = [...]
        X_scaled = X.copy()
        if numeric_features:
            X_scaled.loc[:, numeric_features] = scaler_features.transform(...)
        for col in categorical_features:
            X_scaled.loc[:, col] = X_scaled[col].apply(lambda x: int(x))
```

**é‡å¤ä½ç½®:**
- univariate_single_multi_step_directly_output_forecast (è¡Œ1619-1640)
- univariate_single_multi_step_directly_forecast (è¡Œ1682-1703)
- univariate_single_multi_step_recursive_forecast (è¡Œ1751-1772)
- multivariate_single_multi_step_directly_forecast (è¡Œ1795-1816)
- multivariate_single_multi_step_recursive_forecast (è¡Œ1865-1886)
- å…¶ä»–ä½ç½®...

**ä»£ç é‡å¤åº¦:** çº¦70%

**å½±å“:**
- ç»´æŠ¤å›°éš¾
- ä¿®æ”¹ä¸€å¤„éœ€è¦æ”¹å¤šå¤„
- å®¹æ˜“å‡ºç°ä¸ä¸€è‡´

**ä¼˜å…ˆçº§:** P1ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

---

#### é—®é¢˜3: æ³¨é‡Šè¯­è¨€ä¸ç»Ÿä¸€ ğŸ“

**é—®é¢˜æè¿°:**
- éƒ¨åˆ†æ³¨é‡Šä¸ºè‹±æ–‡
- éƒ¨åˆ†æ³¨é‡Šä¸ºä¸­æ–‡
- ç¼ºä¹ä¸“ä¸šæœ¯è¯­çš„ä¸­è‹±å¯¹ç…§

**ç¤ºä¾‹:**
```python
# è‹±æ–‡æ³¨é‡Š
def load_data(self) -> Dict:
    """
    åŠ è½½æ‰€æœ‰å¿…è¦çš„æ•°æ®  # ä¸­æ–‡
    
    Returns:
        åŒ…å«ç›®æ ‡åºåˆ—ã€æ—¥æœŸç±»å‹ã€å¤©æ°”ç­‰æ•°æ®çš„å­—å…¸  # ä¸­æ–‡
    """
    
# Time delay embedding  # è‹±æ–‡
def extend_lag_feature_multivariate(self, ...):
```

**ä¼˜å…ˆçº§:** P2ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

---

### 2.2 ç‰¹å¾å·¥ç¨‹ä¸è¶³

#### é—®é¢˜4: ç¼ºå°‘æ»åç»Ÿè®¡ç‰¹å¾ ğŸ“Š

**å½“å‰å®ç°:**
- åªæœ‰ç®€å•çš„æ»åç‰¹å¾: `load_lag_1`, `load_lag_2`, ...
- æ²¡æœ‰æ»åçª—å£ç»Ÿè®¡ç‰¹å¾

**ç¼ºå¤±çš„é‡è¦ç‰¹å¾:**

1. **æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾** (Rolling Window Statistics)
   ```python
   # ç¼ºå¤±çš„ç‰¹å¾
   - load_rolling_mean_3   # æœ€è¿‘3æ­¥å¹³å‡å€¼
   - load_rolling_std_7    # æœ€è¿‘7æ­¥æ ‡å‡†å·®
   - load_rolling_min_12   # æœ€è¿‘12æ­¥æœ€å°å€¼
   - load_rolling_max_12   # æœ€è¿‘12æ­¥æœ€å¤§å€¼
   ```

2. **æ‰©å±•çª—å£ç»Ÿè®¡ç‰¹å¾** (Expanding Window Statistics)
   ```python
   - load_expanding_mean   # ç´¯ç§¯å¹³å‡å€¼
   - load_expanding_std    # ç´¯ç§¯æ ‡å‡†å·®
   ```

3. **å·®åˆ†ç‰¹å¾** (Difference Features)
   ```python
   - load_diff_1          # ä¸€é˜¶å·®åˆ†
   - load_diff_seasonal   # å­£èŠ‚æ€§å·®åˆ†
   ```

4. **æ—¶é—´è·ç¦»ç‰¹å¾** (Time-based Features)
   ```python
   - time_since_peak      # è·ç¦»å³°å€¼çš„æ—¶é—´
   - time_since_low       # è·ç¦»è°·å€¼çš„æ—¶é—´
   ```

**å½±å“:**
- æ¨¡å‹æ— æ³•æ•æ‰æ•°æ®çš„ç»Ÿè®¡ç‰¹æ€§
- é¢„æµ‹ç²¾åº¦å—é™

**ä¼˜å…ˆçº§:** P1ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

---

#### é—®é¢˜5: ç¼ºå°‘äº¤äº’ç‰¹å¾ ğŸ”—

**å½“å‰å®ç°:**
- ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹
- æ²¡æœ‰ç‰¹å¾äº¤å‰

**ç¼ºå¤±çš„äº¤äº’ç‰¹å¾:**
```python
# æ—¶é—´ Ã— æ»åå€¼
hour_x_load_lag_1 = hour * load_lag_1

# æ¸©åº¦ Ã— æ¹¿åº¦
temp_x_humidity = temperature * humidity

# å¤šé¡¹å¼ç‰¹å¾
load_lag_1_squared = load_lag_1 ** 2
```

**ä¼˜å…ˆçº§:** P2ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

---

### 2.3 æ¨¡å‹è®­ç»ƒé—®é¢˜

#### é—®é¢˜6: ç¼ºå°‘æ¨¡å‹èåˆ ğŸ¯

**å½“å‰å®ç°:**
- åªä½¿ç”¨å•ä¸€æ¨¡å‹ï¼ˆLightGBMï¼‰
- æ²¡æœ‰æ¨¡å‹é›†æˆç­–ç•¥

**ç¼ºå¤±çš„èåˆæ–¹æ³•:**

1. **Votingï¼ˆæŠ•ç¥¨ï¼‰**
   - Hard Votingï¼ˆç¡¬æŠ•ç¥¨ï¼‰
   - Soft Votingï¼ˆè½¯æŠ•ç¥¨ï¼‰

2. **Averagingï¼ˆå¹³å‡ï¼‰**
   - Simple Averageï¼ˆç®€å•å¹³å‡ï¼‰
   - Weighted Averageï¼ˆåŠ æƒå¹³å‡ï¼‰

3. **Stackingï¼ˆå †å ï¼‰**
   - ä¸¤å±‚å †å 
   - å¤šå±‚å †å 

4. **Blendingï¼ˆæ··åˆï¼‰**

**é¢„æœŸæå‡:** 5-15%

**ä¼˜å…ˆçº§:** P1ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

---

#### é—®é¢˜7: è¶…å‚æ•°è°ƒä¼˜ä¸å®Œå–„ âš™ï¸

**å½“å‰å®ç°:**
```python
# è¡Œ1477-1527: å­˜åœ¨è¶…å‚æ•°è°ƒä¼˜ä»£ç 
def _hyperparameters_tuning(self, X_train, Y_train):
    param_grid = {
        'estimator__num_leaves': [15, 31, 63],
        'estimator__learning_rate': [0.01, 0.05, 0.1],
        # ...
    }
```

**é—®é¢˜:**
- å‚æ•°ç½‘æ ¼è¿‡äºç®€å•
- æ²¡æœ‰ä½¿ç”¨Optunaç­‰é«˜çº§ä¼˜åŒ–å·¥å…·
- æ²¡æœ‰æ—©åœç­–ç•¥ä¼˜åŒ–

**ä¼˜å…ˆçº§:** P2ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

---

### 2.4 æ¨¡å‹è¯„ä¼°é—®é¢˜

#### é—®é¢˜8: è¯„ä¼°æŒ‡æ ‡å•ä¸€ ğŸ“ˆ

**å½“å‰æŒ‡æ ‡:**
- R2, MSE, RMSE, MAE, MAPE

**ç¼ºå¤±çš„æ—¶é—´åºåˆ—ç‰¹å®šæŒ‡æ ‡:**
- SMAPE (Symmetric MAPE)
- WAPE (Weighted APE)
- sMAPE
- Directional Accuracy (æ–¹å‘å‡†ç¡®ç‡)
- Peak Error (å³°å€¼è¯¯å·®)

**ä¼˜å…ˆçº§:** P3ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

---

### 2.5 ä»£ç è‡ƒè‚¿é—®é¢˜

#### é—®é¢˜9: æ–¹æ³•è¿‡é•¿ ğŸ“

**è¶…é•¿æ–¹æ³•:**
```python
run() æ–¹æ³•: ~200è¡Œ
train() æ–¹æ³•: ~100è¡Œ
å„ä¸ªé¢„æµ‹æ–¹æ³•: 50-100è¡Œ/æ–¹æ³•
```

**é—®é¢˜:**
- å¯è¯»æ€§å·®
- éš¾ä»¥æµ‹è¯•
- è¿åå•ä¸€èŒè´£åŸåˆ™

**ä¼˜å…ˆçº§:** P1ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

---

## 3. ä¼˜åŒ–æ–¹æ¡ˆæ€»è§ˆ ğŸš€

### 3.1 æ¶æ„ä¼˜åŒ–æ–¹æ¡ˆ

#### ä¼˜åŒ–1: æ¨¡å‹æŠ½è±¡åŒ–ï¼ˆè®¾è®¡æ¨¡å¼ï¼‰

**ç›®æ ‡:** å®ç°æ¨¡å‹å¯æ’æ‹”

**è®¾è®¡æ–¹æ¡ˆ:**

```python
# 1. å®šä¹‰æ¨¡å‹åŸºç±»
class BaseModel(ABC):
    """æ¨¡å‹åŸºç±»"""
    
    @abstractmethod
    def fit(self, X, y, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X, **kwargs):
        """é¢„æµ‹"""
        pass
    
    @abstractmethod
    def get_params(self):
        """è·å–å‚æ•°"""
        pass

# 2. å…·ä½“æ¨¡å‹å®ç°
class LightGBMModel(BaseModel):
    """LightGBMæ¨¡å‹"""
    def __init__(self, params):
        self.params = params
        self.model = lgb.LGBMRegressor(**params)
    
    def fit(self, X, y, **kwargs):
        self.model.fit(X, y, **kwargs)
        return self
    
    def predict(self, X, **kwargs):
        return self.model.predict(X, **kwargs)

class XGBoostModel(BaseModel):
    """XGBoostæ¨¡å‹"""
    def __init__(self, params):
        self.params = params
        self.model = xgb.XGBRegressor(**params)
    
    def fit(self, X, y, **kwargs):
        self.model.fit(X, y, **kwargs)
        return self
    
    def predict(self, X, **kwargs):
        return self.model.predict(X, **kwargs)

class CatBoostModel(BaseModel):
    """CatBoostæ¨¡å‹"""
    def __init__(self, params):
        self.params = params
        self.model = cab.CatBoostRegressor(**params)
    
    def fit(self, X, y, **kwargs):
        self.model.fit(X, y, **kwargs)
        return self
    
    def predict(self, X, **kwargs):
        return self.model.predict(X, **kwargs)

# 3. æ¨¡å‹å·¥å‚
class ModelFactory:
    """æ¨¡å‹å·¥å‚"""
    
    @staticmethod
    def create_model(model_type: str, params: dict) -> BaseModel:
        models = {
            'lightgbm': LightGBMModel,
            'xgboost': XGBoostModel,
            'catboost': CatBoostModel,
        }
        
        if model_type not in models:
            raise ValueError(f"Unknown model type: {model_type}")
        
        return models[model_type](params)

# 4. åœ¨Modelç±»ä¸­ä½¿ç”¨
class Model:
    def __init__(self, args):
        # ...
        # ä½¿ç”¨å·¥å‚åˆ›å»ºæ¨¡å‹
        self.model_factory = ModelFactory()
    
    def train(self, X_train, Y_train, categorical_features):
        # åˆ›å»ºæ¨¡å‹
        base_model = self.model_factory.create_model(
            self.args.model_type,  # 'lightgbm' / 'xgboost' / 'catboost'
            self.model_params
        )
        
        # è®­ç»ƒ
        if self.args.pred_method in [...]:
            model = base_model
        else:
            model = MultiOutputRegressor(base_model)
        
        model.fit(X_train, Y_train)
        return model
```

**ä¼˜åŠ¿:**
- âœ… è½»æ¾åˆ‡æ¢æ¨¡å‹
- âœ… æ˜“äºæ‰©å±•æ–°æ¨¡å‹
- âœ… ç¬¦åˆè®¾è®¡æ¨¡å¼
- âœ… ä»£ç æ›´æ¸…æ™°

---

#### ä¼˜åŒ–2: ç‰¹å¾å·¥ç¨‹æ¨¡å—åŒ–

**ç›®æ ‡:** å‡å°‘ä»£ç é‡å¤ï¼Œæå‡å¯ç»´æŠ¤æ€§

**è®¾è®¡æ–¹æ¡ˆ:**

```python
class EnhancedFeaturePreprocessor(FeaturePreprocessor):
    """å¢å¼ºçš„ç‰¹å¾é¢„å¤„ç†å™¨"""
    
    def add_lag_statistics(self, df: pd.DataFrame, column: str, windows: List[int]):
        """
        æ·»åŠ æ»åç»Ÿè®¡ç‰¹å¾
        
        Args:
            df: æ•°æ®æ¡†
            column: åˆ—å
            windows: çª—å£å¤§å°åˆ—è¡¨ [3, 7, 14]
        
        Returns:
            å¢å¼ºåçš„æ•°æ®æ¡†
        """
        df_enhanced = df.copy()
        
        for window in windows:
            # æ»‘åŠ¨å¹³å‡
            df_enhanced[f'{column}_rolling_mean_{window}'] = (
                df[column].rolling(window=window, min_periods=1).mean()
            )
            
            # æ»‘åŠ¨æ ‡å‡†å·®
            df_enhanced[f'{column}_rolling_std_{window}'] = (
                df[column].rolling(window=window, min_periods=1).std()
            )
            
            # æ»‘åŠ¨æœ€å°å€¼
            df_enhanced[f'{column}_rolling_min_{window}'] = (
                df[column].rolling(window=window, min_periods=1).min()
            )
            
            # æ»‘åŠ¨æœ€å¤§å€¼
            df_enhanced[f'{column}_rolling_max_{window}'] = (
                df[column].rolling(window=window, min_periods=1).max()
            )
            
            # æ»‘åŠ¨ä¸­ä½æ•°
            df_enhanced[f'{column}_rolling_median_{window}'] = (
                df[column].rolling(window=window, min_periods=1).median()
            )
        
        return df_enhanced
    
    def add_diff_features(self, df: pd.DataFrame, column: str, periods: List[int]):
        """
        æ·»åŠ å·®åˆ†ç‰¹å¾
        
        Args:
            df: æ•°æ®æ¡†
            column: åˆ—å
            periods: å·®åˆ†å‘¨æœŸåˆ—è¡¨ [1, 7, 24]
        """
        df_enhanced = df.copy()
        
        for period in periods:
            df_enhanced[f'{column}_diff_{period}'] = df[column].diff(period)
        
        return df_enhanced
    
    def add_expanding_features(self, df: pd.DataFrame, column: str):
        """æ·»åŠ æ‰©å±•çª—å£ç‰¹å¾"""
        df_enhanced = df.copy()
        
        df_enhanced[f'{column}_expanding_mean'] = df[column].expanding().mean()
        df_enhanced[f'{column}_expanding_std'] = df[column].expanding().std()
        df_enhanced[f'{column}_expanding_min'] = df[column].expanding().min()
        df_enhanced[f'{column}_expanding_max'] = df[column].expanding().max()
        
        return df_enhanced
    
    def add_time_based_features(self, df: pd.DataFrame, column: str, time_column: str):
        """æ·»åŠ åŸºäºæ—¶é—´çš„ç‰¹å¾"""
        df_enhanced = df.copy()
        
        # è·ç¦»å³°å€¼çš„æ—¶é—´
        peak_idx = df[column].idxmax()
        df_enhanced[f'{column}_time_since_peak'] = (df.index - peak_idx).total_seconds() / 3600
        
        # è·ç¦»è°·å€¼çš„æ—¶é—´
        low_idx = df[column].idxmin()
        df_enhanced[f'{column}_time_since_low'] = (df.index - low_idx).total_seconds() / 3600
        
        return df_enhanced
```

---

#### ä¼˜åŒ–3: ç»Ÿä¸€é¢„å¤„ç†æ¥å£

**ç›®æ ‡:** æ¶ˆé™¤é‡å¤ä»£ç 

**è®¾è®¡æ–¹æ¡ˆ:**

```python
class FeatureScaler:
    """ç»Ÿä¸€çš„ç‰¹å¾ç¼©æ”¾å™¨"""
    
    def __init__(self, scaler, encode_categorical: bool = False):
        self.scaler = scaler
        self.encode_categorical = encode_categorical
        self.category_encoders = {}
    
    def fit_transform(self, X: pd.DataFrame, categorical_features: List[str]):
        """è®­ç»ƒå¹¶è½¬æ¢"""
        X_scaled = X.copy()
        
        # åˆ†ç¦»æ•°å€¼å’Œç±»åˆ«ç‰¹å¾
        numeric_features = [col for col in X.columns if col not in categorical_features]
        
        # ç¼©æ”¾æ•°å€¼ç‰¹å¾
        if numeric_features and self.scaler is not None:
            X_scaled[numeric_features] = self.scaler.fit_transform(X[numeric_features])
        
        # ç¼–ç ç±»åˆ«ç‰¹å¾
        if self.encode_categorical:
            for col in categorical_features:
                if col in X.columns:
                    X_scaled[col] = X[col].astype('category')
                    self.category_encoders[col] = {
                        'categories': X_scaled[col].cat.categories.tolist()
                    }
                    X_scaled[col] = X_scaled[col].cat.codes
        
        return X_scaled
    
    def transform(self, X: pd.DataFrame, categorical_features: List[str]):
        """ä»…è½¬æ¢"""
        X_scaled = X.copy()
        
        # åˆ†ç¦»æ•°å€¼å’Œç±»åˆ«ç‰¹å¾
        numeric_features = [col for col in X.columns if col not in categorical_features]
        
        # ç¼©æ”¾æ•°å€¼ç‰¹å¾
        if numeric_features and self.scaler is not None:
            X_scaled[numeric_features] = self.scaler.transform(X[numeric_features])
        
        # ç¼–ç ç±»åˆ«ç‰¹å¾
        if self.encode_categorical:
            for col in categorical_features:
                if col in X.columns and col in self.category_encoders:
                    encoder_info = self.category_encoders[col]
                    X_scaled[col] = pd.Categorical(
                        X[col],
                        categories=encoder_info['categories']
                    )
                    X_scaled[col] = X_scaled[col].cat.codes
        
        return X_scaled
```

**ä½¿ç”¨ç¤ºä¾‹:**
```python
# è®­ç»ƒæ—¶
scaler = FeatureScaler(StandardScaler(), encode_categorical=True)
X_train_scaled = scaler.fit_transform(X_train, categorical_features)

# é¢„æµ‹æ—¶
X_test_scaled = scaler.transform(X_test, categorical_features)
```

**ä¼˜åŠ¿:**
- âœ… ä»£ç é‡å¤ä»70%é™åˆ°0%
- âœ… ä¸€å¤„ä¿®æ”¹ï¼Œå¤„å¤„ç”Ÿæ•ˆ
- âœ… æ›´æ˜“ç»´æŠ¤

---

### 3.2 æ¨¡å‹èåˆæ–¹æ¡ˆ

#### ä¼˜åŒ–4: å®ç°å¤šç§èåˆç­–ç•¥

**å®ç°æ–¹æ¡ˆ:**

```python
class ModelEnsemble:
    """æ¨¡å‹èåˆå™¨"""
    
    def __init__(self, models: List[BaseModel], method: str = 'voting'):
        """
        åˆå§‹åŒ–
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            method: èåˆæ–¹æ³• ['voting', 'averaging', 'stacking', 'blending']
        """
        self.models = models
        self.method = method
        self.meta_model = None  # ç”¨äºstacking
    
    def fit(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        # è®­ç»ƒåŸºæ¨¡å‹
        for i, model in enumerate(self.models):
            logger.info(f"Training model {i+1}/{len(self.models)}")
            model.fit(X_train, y_train)
        
        # å¦‚æœæ˜¯stackingï¼Œè®­ç»ƒå…ƒæ¨¡å‹
        if self.method == 'stacking' and X_val is not None:
            # è·å–åŸºæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„é¢„æµ‹
            meta_features = self._get_meta_features(X_val)
            
            # è®­ç»ƒå…ƒæ¨¡å‹
            self.meta_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05)
            self.meta_model.fit(meta_features, y_val)
    
    def predict(self, X):
        """èåˆé¢„æµ‹"""
        if self.method == 'voting':
            return self._voting_predict(X)
        elif self.method == 'averaging':
            return self._averaging_predict(X)
        elif self.method == 'stacking':
            return self._stacking_predict(X)
        elif self.method == 'blending':
            return self._blending_predict(X)
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def _voting_predict(self, X):
        """æŠ•ç¥¨é¢„æµ‹ï¼ˆç”¨äºåˆ†ç±»ï¼Œå›å½’ç”¨å¹³å‡ï¼‰"""
        return self._averaging_predict(X)
    
    def _averaging_predict(self, X):
        """å¹³å‡é¢„æµ‹"""
        predictions = np.array([model.predict(X) for model in self.models])
        return predictions.mean(axis=0)
    
    def _weighted_averaging_predict(self, X, weights):
        """åŠ æƒå¹³å‡é¢„æµ‹"""
        predictions = np.array([model.predict(X) for model in self.models])
        return np.average(predictions, axis=0, weights=weights)
    
    def _stacking_predict(self, X):
        """å †å é¢„æµ‹"""
        # è·å–åŸºæ¨¡å‹é¢„æµ‹ä½œä¸ºå…ƒç‰¹å¾
        meta_features = self._get_meta_features(X)
        
        # ä½¿ç”¨å…ƒæ¨¡å‹é¢„æµ‹
        return self.meta_model.predict(meta_features)
    
    def _blending_predict(self, X):
        """æ··åˆé¢„æµ‹ï¼ˆç±»ä¼¼stackingä½†ä½¿ç”¨å›ºå®šæƒé‡ï¼‰"""
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨å¹³å‡
        return self._averaging_predict(X)
    
    def _get_meta_features(self, X):
        """è·å–å…ƒç‰¹å¾ï¼ˆåŸºæ¨¡å‹çš„é¢„æµ‹ï¼‰"""
        meta_features = []
        for model in self.models:
            pred = model.predict(X)
            meta_features.append(pred)
        
        return np.column_stack(meta_features)

# ä½¿ç”¨ç¤ºä¾‹
def train_with_ensemble(X_train, y_train, X_val, y_val):
    # åˆ›å»ºå¤šä¸ªæ¨¡å‹
    models = [
        LightGBMModel({'n_estimators': 1000, 'learning_rate': 0.05}),
        XGBoostModel({'n_estimators': 1000, 'learning_rate': 0.05}),
        CatBoostModel({'iterations': 1000, 'learning_rate': 0.05}),
    ]
    
    # åˆ›å»ºèåˆå™¨
    ensemble = ModelEnsemble(models, method='stacking')
    
    # è®­ç»ƒ
    ensemble.fit(X_train, y_train, X_val, y_val)
    
    # é¢„æµ‹
    y_pred = ensemble.predict(X_val)
    
    return ensemble, y_pred
```

**é¢„æœŸæå‡:** 10-20%

---

### 3.3 ç‰¹å¾å·¥ç¨‹å¢å¼º

#### ä¼˜åŒ–5: æ·»åŠ é«˜çº§ç‰¹å¾

**å®ç°æ¸…å•:**

1. âœ… æ»‘åŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
2. âœ… å·®åˆ†ç‰¹å¾
3. âœ… æ‰©å±•çª—å£ç‰¹å¾
4. âœ… æ—¶é—´è·ç¦»ç‰¹å¾
5. âœ… äº¤äº’ç‰¹å¾
6. âœ… å¤šé¡¹å¼ç‰¹å¾

**ä»£ç ç¤ºä¾‹è§ä¼˜åŒ–2**

---

## 4. æ€§èƒ½æå‡å»ºè®® ğŸ¯

### 4.1 æ¨¡å‹å±‚é¢

#### å»ºè®®1: ä½¿ç”¨æ›´å¥½çš„æŸå¤±å‡½æ•°

```python
# å½“å‰: MAE æˆ– MSE
# å»ºè®®: Huber Loss (å¯¹å¼‚å¸¸å€¼æ›´é²æ£’)

from sklearn.metrics import make_scorer

def huber_loss(y_true, y_pred, delta=1.0):
    error = y_true - y_pred
    is_small_error = np.abs(error) <= delta
    squared_loss = 0.5 * error ** 2
    linear_loss = delta * (np.abs(error) - 0.5 * delta)
    return np.where(is_small_error, squared_loss, linear_loss).mean()
```

#### å»ºè®®2: åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡

```python
# ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
import optuna

def objective(trial):
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
    }
    
    model = lgb.LGBMRegressor(**params)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_val)
    return mean_absolute_error(y_val, y_pred)

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
```

#### å»ºè®®3: ä½¿ç”¨äº¤å‰éªŒè¯

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]
    
    # è®­ç»ƒå’Œè¯„ä¼°
    model.fit(X_train, y_train)
    score = model.score(X_val, y_val)
```

---

### 4.2 ç‰¹å¾å±‚é¢

#### å»ºè®®4: ç‰¹å¾é€‰æ‹©

```python
from sklearn.feature_selection import SelectKBest, f_regression

# é€‰æ‹©æœ€é‡è¦çš„Kä¸ªç‰¹å¾
selector = SelectKBest(score_func=f_regression, k=50)
X_selected = selector.fit_transform(X_train, y_train)
```

#### å»ºè®®5: ç‰¹å¾é‡è¦æ€§åˆ†æ

```python
# è®­ç»ƒåæŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
feature_importance = model.feature_importances_
feature_names = X_train.columns

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

# åªä¿ç•™é‡è¦ç‰¹å¾
top_features = importance_df.head(50)['feature'].tolist()
X_train_selected = X_train[top_features]
```

---

### 4.3 æ•°æ®å±‚é¢

#### å»ºè®®6: æ•°æ®å¢å¼º

```python
# æ—¶é—´åºåˆ—æ•°æ®å¢å¼º
def augment_time_series(df, noise_level=0.01):
    """æ·»åŠ å™ªå£°å¢å¼º"""
    df_augmented = df.copy()
    noise = np.random.normal(0, noise_level, len(df))
    df_augmented['target'] = df['target'] + df['target'] * noise
    return df_augmented
```

#### å»ºè®®7: å¼‚å¸¸å€¼å¤„ç†

```python
from scipy import stats

def remove_outliers(df, column, threshold=3):
    """ç§»é™¤å¼‚å¸¸å€¼ï¼ˆZ-scoreæ–¹æ³•ï¼‰"""
    z_scores = np.abs(stats.zscore(df[column]))
    return df[z_scores < threshold]
```

---

## 5. ä»£ç é‡æ„å»ºè®® ğŸ”¨

### 5.1 æ–¹æ³•æ‹†åˆ†

**é‡æ„å‰:**
```python
def run(self):
    # 200è¡Œä»£ç ...
    pass
```

**é‡æ„å:**
```python
def run(self):
    # æ•°æ®åŠ è½½
    data = self._load_and_prepare_data()
    
    # ç‰¹å¾å·¥ç¨‹
    features = self._engineer_features(data)
    
    # æ¨¡å‹è®­ç»ƒ
    model = self._train_model(features)
    
    # æ¨¡å‹æµ‹è¯•
    test_results = self._test_model(model, features)
    
    # æ¨¡å‹é¢„æµ‹
    predictions = self._make_predictions(model, features)
    
    # ä¿å­˜ç»“æœ
    self._save_results(test_results, predictions)

def _load_and_prepare_data(self):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    # 50è¡Œä»£ç 
    pass

def _engineer_features(self, data):
    """ç‰¹å¾å·¥ç¨‹"""
    # 50è¡Œä»£ç 
    pass

# ... å…¶ä»–å­æ–¹æ³•
```

---

### 5.2 é…ç½®åˆ†ç¦»

**å»ºè®®:** å°†é…ç½®ä»ä»£ç ä¸­åˆ†ç¦»åˆ°YAMLæ–‡ä»¶

```yaml
# config.yaml
model:
  type: lightgbm
  params:
    n_estimators: 1000
    learning_rate: 0.05
    max_depth: 7

features:
  lags: [1, 2, 7, 14, 21]
  rolling_windows: [3, 7, 14]
  diff_periods: [1, 7]

training:
  window_days: 30
  predict_days: 1
  test_windows: 5
```

**åŠ è½½é…ç½®:**
```python
import yaml

with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model_type = config['model']['type']
model_params = config['model']['params']
```

---

## 6. ç»Ÿè®¡æ¨¡å‹æ¡†æ¶è®¾è®¡ ğŸ“ˆ

### 6.1 æ”¯æŒçš„ç»Ÿè®¡æ¨¡å‹

å»ºè®®å®ç°ä»¥ä¸‹ç»Ÿè®¡æ¨¡å‹ï¼š

1. **ARIMA** (AutoRegressive Integrated Moving Average)
2. **SARIMA** (Seasonal ARIMA)
3. **ETS** (Exponential Smoothing)
4. **Prophet** (Facebook Prophet)
5. **VAR** (Vector AutoRegression) - å¤šå˜é‡
6. **Theta Method**

### 6.2 æ¡†æ¶ç»“æ„

```python
# exp_forecasting_stat.py

class StatisticalModel(ABC):
    """ç»Ÿè®¡æ¨¡å‹åŸºç±»"""
    
    @abstractmethod
    def fit(self, y, exog=None):
        """æ‹Ÿåˆæ¨¡å‹"""
        pass
    
    @abstractmethod
    def forecast(self, steps, exog=None):
        """é¢„æµ‹"""
        pass

class ARIMAModel(StatisticalModel):
    """ARIMAæ¨¡å‹"""
    
    def __init__(self, order=(1, 1, 1)):
        from statsmodels.tsa.arima.model import ARIMA
        self.order = order
        self.model = None
        self.fitted_model = None
    
    def fit(self, y, exog=None):
        from statsmodels.tsa.arima.model import ARIMA
        self.model = ARIMA(y, order=self.order, exog=exog)
        self.fitted_model = self.model.fit()
        return self
    
    def forecast(self, steps, exog=None):
        return self.fitted_model.forecast(steps=steps, exog=exog)

class SARIMAModel(StatisticalModel):
    """SARIMAæ¨¡å‹"""
    
    def __init__(self, order=(1,1,1), seasonal_order=(1,1,1,12)):
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        self.order = order
        self.seasonal_order = seasonal_order
        self.model = None
        self.fitted_model = None
    
    def fit(self, y, exog=None):
        from statsmodels.tsa.statespace.sarimax import SARIMAX
        self.model = SARIMAX(
            y, 
            order=self.order,
            seasonal_order=self.seasonal_order,
            exog=exog
        )
        self.fitted_model = self.model.fit(disp=False)
        return self
    
    def forecast(self, steps, exog=None):
        return self.fitted_model.forecast(steps=steps, exog=exog)

class ProphetModel(StatisticalModel):
    """Prophetæ¨¡å‹"""
    
    def __init__(self, **kwargs):
        from prophet import Prophet
        self.model = Prophet(**kwargs)
        self.fitted = False
    
    def fit(self, y, exog=None):
        # Prophetéœ€è¦ç‰¹å®šçš„æ•°æ®æ ¼å¼
        df = pd.DataFrame({'ds': y.index, 'y': y.values})
        
        if exog is not None:
            for col in exog.columns:
                df[col] = exog[col].values
                self.model.add_regressor(col)
        
        self.model.fit(df)
        self.fitted = True
        return self
    
    def forecast(self, steps, exog=None):
        # åˆ›å»ºæœªæ¥æ—¥æœŸ
        future = self.model.make_future_dataframe(periods=steps)
        
        if exog is not None:
            for col in exog.columns:
                future[col] = exog[col].values
        
        forecast = self.model.predict(future)
        return forecast['yhat'].tail(steps).values
```

---

## 7. ä¼˜åŒ–ä¼˜å…ˆçº§æ€»ç»“ ğŸ“‹

### P0 ä¼˜å…ˆçº§ï¼ˆç«‹å³ä¿®å¤ï¼‰

1. âœ… **æ¨¡å‹æŠ½è±¡åŒ–** - è§£é™¤ç¡¬ç¼–ç 
2. âœ… **ç»Ÿä¸€é¢„å¤„ç†æ¥å£** - æ¶ˆé™¤é‡å¤ä»£ç 

### P1 ä¼˜å…ˆçº§ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

3. âœ… **æ·»åŠ æ»åç»Ÿè®¡ç‰¹å¾** - æå‡ç²¾åº¦10-15%
4. âœ… **æ¨¡å‹èåˆ** - æå‡ç²¾åº¦5-15%
5. âœ… **æ–¹æ³•æ‹†åˆ†** - æå‡å¯ç»´æŠ¤æ€§

### P2 ä¼˜å…ˆçº§ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

6. âœ… **ä¼˜åŒ–æ³¨é‡Š** - ä¸­è‹±æ–‡ç»Ÿä¸€
7. âœ… **è¶…å‚æ•°è°ƒä¼˜æ”¹è¿›** - ä½¿ç”¨Optuna
8. âœ… **æ·»åŠ äº¤äº’ç‰¹å¾** - æå‡ç²¾åº¦3-5%

### P3 ä¼˜å…ˆçº§ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

9. âœ… **å¢åŠ è¯„ä¼°æŒ‡æ ‡** - æ›´å…¨é¢çš„è¯„ä¼°
10. âœ… **é…ç½®åˆ†ç¦»** - æ›´çµæ´»çš„é…ç½®ç®¡ç†

---

## 8. é¢„æœŸæˆæœ ğŸ¯

### 8.1 ä»£ç è´¨é‡æå‡

- ä»£ç è¡Œæ•°å‡å°‘: 2761 â†’ ~2000 (-27%)
- ä»£ç é‡å¤åº¦: 70% â†’ 0% (-100%)
- æ–¹æ³•å¹³å‡é•¿åº¦: 100è¡Œ â†’ 30è¡Œ (-70%)
- å¯æµ‹è¯•æ€§: ä½ â†’ é«˜

### 8.2 æ€§èƒ½æå‡

- ç‰¹å¾å·¥ç¨‹å¢å¼º: **+10-15%**
- æ¨¡å‹èåˆ: **+5-15%**
- è¶…å‚æ•°ä¼˜åŒ–: **+3-5%**
- **æ€»ä½“é¢„æœŸæå‡: 20-35%**

### 8.3 å¯ç»´æŠ¤æ€§æå‡

- æ¨¡å—åŒ–ç¨‹åº¦: ä¸­ â†’ é«˜
- æ‰©å±•æ€§: ä¸­ â†’ æé«˜
- å¯è¯»æ€§: ä¸­ â†’ é«˜
- æ–‡æ¡£å®Œæ•´æ€§: ä¸­ â†’ é«˜

---

## 9. å®æ–½å»ºè®® ğŸ”§

### é˜¶æ®µ1: æ ¸å¿ƒé‡æ„ï¼ˆ1å‘¨ï¼‰

- [ ] å®ç°æ¨¡å‹æŠ½è±¡åŒ–
- [ ] ç»Ÿä¸€é¢„å¤„ç†æ¥å£
- [ ] æ‹†åˆ†å¤§æ–¹æ³•

### é˜¶æ®µ2: ç‰¹å¾å¢å¼ºï¼ˆ3å¤©ï¼‰

- [ ] æ·»åŠ æ»åç»Ÿè®¡ç‰¹å¾
- [ ] æ·»åŠ å·®åˆ†ç‰¹å¾
- [ ] æ·»åŠ äº¤äº’ç‰¹å¾

### é˜¶æ®µ3: æ¨¡å‹èåˆï¼ˆ3å¤©ï¼‰

- [ ] å®ç°ModelEnsembleç±»
- [ ] æµ‹è¯•ä¸åŒèåˆç­–ç•¥
- [ ] æ€§èƒ½å¯¹æ¯”

### é˜¶æ®µ4: ç»Ÿè®¡æ¨¡å‹æ¡†æ¶ï¼ˆ1å‘¨ï¼‰

- [ ] å®ç°StatisticalModelåŸºç±»
- [ ] å®ç°ARIMA, SARIMA, Prophetç­‰
- [ ] å®Œæ•´æµ‹è¯•

### é˜¶æ®µ5: ä¼˜åŒ–å’Œæµ‹è¯•ï¼ˆ3å¤©ï¼‰

- [ ] ä»£ç å®¡æŸ¥
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] æ–‡æ¡£å®Œå–„

**æ€»è®¡: çº¦3å‘¨**

---

## 10. æ€»ç»“ âœ¨

### å½“å‰æ¡†æ¶è¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| åŠŸèƒ½å®Œæ•´æ€§ | â­â­â­â­â­ | 7ç§æ–¹æ³•å…¨è¦†ç›– |
| ä»£ç è´¨é‡ | â­â­â­ | å­˜åœ¨é‡å¤å’Œç¡¬ç¼–ç  |
| æ€§èƒ½ | â­â­â­â­ | åŸºç¡€æ€§èƒ½è‰¯å¥½ |
| æ‰©å±•æ€§ | â­â­ | æ¨¡å‹ç¡¬ç¼–ç é™åˆ¶æ‰©å±• |
| å¯ç»´æŠ¤æ€§ | â­â­â­ | ä»£ç é‡å¤å½±å“ç»´æŠ¤ |
| **æ€»åˆ†** | **â­â­â­â­** | **è‰¯å¥½ï¼Œæœ‰æå‡ç©ºé—´** |

### ä¼˜åŒ–åé¢„æœŸè¯„åˆ†

| ç»´åº¦ | è¯„åˆ† | æå‡ |
|------|------|------|
| åŠŸèƒ½å®Œæ•´æ€§ | â­â­â­â­â­ | - |
| ä»£ç è´¨é‡ | â­â­â­â­â­ | +2â­ |
| æ€§èƒ½ | â­â­â­â­â­ | +1â­ |
| æ‰©å±•æ€§ | â­â­â­â­â­ | +3â­ |
| å¯ç»´æŠ¤æ€§ | â­â­â­â­â­ | +2â­ |
| **æ€»åˆ†** | **â­â­â­â­â­** | **+1â­** |

---

**æŠ¥å‘Šå®Œæˆæ—¥æœŸ**: 2026-02-11  
**å»ºè®®å®æ–½ä¼˜å…ˆçº§**: P0 â†’ P1 â†’ P2 â†’ P3  
**é¢„æœŸå®Œæˆå‘¨æœŸ**: 3å‘¨  
**é¢„æœŸæ€§èƒ½æå‡**: 20-35%
